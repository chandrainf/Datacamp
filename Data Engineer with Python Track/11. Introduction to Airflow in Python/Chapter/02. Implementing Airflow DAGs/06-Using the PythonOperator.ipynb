'''
Using the PythonOperator


You've implemented several Airflow tasks using the BashOperator but realize that a couple of specific tasks would be better implemented using Python. You'll implement a task to download and save a file to the system within Airflow.

The requests library is imported for you, and the DAG process_sales_dag is already defined.

Instructions 1/3
35 XP

- Define a function called pull_file with two parameters, URL and savepath.
- Use the print() function and Python f-strings to write a message to the logs.

'''
# Define the method

def pull_file(URL, savepath):
    r = requests.get(URL)
    with open(savepath, 'wb') as f:
        f.write(r.content)
    # Use the print method for logging
    print(f"File pulled from {URL} and saved to {savepath}")





'''
Instructions 2/3
35 XP

-Import the necessary object to use the Python Operator.

'''

def pull_file(URL, savepath):
    r = requests.get(URL)
    with open(savepath, 'wb') as f:
        f.write(r.content)
    # Use the print method for logging
    print(f"File pulled from {URL} and saved to {savepath}")


# Import the PythonOperator class
from airflow.operators.python_operator import PythonOperator






'''
Instructions 3/3
30 XP

- Create a new task assigned to the variable pull_file_task, with the id pull_file.
- Add the pull_file(URL, savepath) function defined previously to the operator.
- Define the arguments needed for the task.

'''
def pull_file(URL, savepath):
    r = requests.get(URL)
    with open(savepath, 'wb') as f:
        f.write(r.content)
    # Use the print method for logging
    print(f"File pulled from {URL} and saved to {savepath}")


# Create the task
pull_file_task = PythonOperator(
    task_id='pull_file',
    # Add the callable
    python_callable=pull_file,
    # Define the arguments
    op_kwargs={'URL': 'http://dataserver/sales.json',
               'savepath': 'latestsales.json'},
    dag=process_sales_dag
)
